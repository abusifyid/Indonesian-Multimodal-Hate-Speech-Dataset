{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgQk0DpG9j01"
      },
      "source": [
        "## Machine Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XwHOXj0O5Ma"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, VotingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import cross_val_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhKP0-F-5_Sg"
      },
      "outputs": [],
      "source": [
        "def load_and_preprocess_images(image_folder, image_names, target_size=(224, 224)):\n",
        "    features = []\n",
        "    for image_name in image_names:\n",
        "        image_path = os.path.join(image_folder, image_name)\n",
        "        image = cv2.imread(image_path)\n",
        "        if image is not None:\n",
        "            image = cv2.resize(image, target_size)\n",
        "            image = image / 255.0\n",
        "            image_array = image.flatten()\n",
        "            features.append(image_array)\n",
        "    return np.array(features)\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('path_to_dataset/multimodal_tweets_dataset.csv')\n",
        "df['anotasi'] = df['anotasi'].map({'not hate': 0, 'hate': 1})\n",
        "\n",
        "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "\n",
        "for train_index, test_index in sss.split(df, df['anotasi']):\n",
        "    df_train = df.iloc[train_index]\n",
        "    df_test = df.iloc[test_index]\n",
        "\n",
        "train_images = df_train['images'].tolist()\n",
        "train_texts = df_train['tweet_text'].tolist()\n",
        "train_labels = df_train['anotasi'].tolist()\n",
        "test_images = df_test['images'].tolist()\n",
        "test_texts = df_test['tweet_text'].tolist()\n",
        "test_labels = df_test['anotasi'].tolist()\n",
        "\n",
        "train_image_features = load_and_preprocess_images(\"/content/drive/MyDrive/Multimodal Research/selected_images/\", train_images)\n",
        "test_image_features = load_and_preprocess_images(\"/content/drive/MyDrive/Multimodal Research/selected_images/\", test_images)\n",
        "\n",
        "cv = CountVectorizer()\n",
        "train_text_features = cv.fit_transform(train_texts).toarray()\n",
        "test_text_features = cv.transform(test_texts).toarray()\n",
        "\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "train_text_features_resampled, train_labels_resampled = ros.fit_resample(train_text_features, train_labels)\n",
        "train_image_features_resampled, train_labels_resampled = ros.fit_resample(train_image_features, train_labels)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_predict_proba(train_features, train_labels, test_features):\n",
        "    model = MultinomialNB()\n",
        "    model.fit(train_features, train_labels)\n",
        "    probas = model.predict_proba(test_features)\n",
        "    return probas\n",
        "\n",
        "text_probas = train_and_predict_proba(train_text_features_resampled, train_labels_resampled, test_text_features)\n",
        "image_probas = train_and_predict_proba(train_image_features_resampled, train_labels_resampled, test_image_features)\n",
        "\n",
        "def soft_voting_predictions(text_probas, image_probas, weights):\n",
        "    combined_probas = (text_probas * weights[0] + image_probas * weights[1])\n",
        "    final_predictions = np.argmax(combined_probas, axis=1)\n",
        "    return final_predictions\n",
        "\n",
        "weights_50_50 = [0.5, 0.5]\n",
        "weights_60_40 = [0.6, 0.4]\n",
        "weights_70_30 = [0.7, 0.3]\n",
        "\n",
        "soft_voting_predictions_50_50 = soft_voting_predictions(text_probas, image_probas, weights_50_50)\n",
        "soft_voting_predictions_60_40 = soft_voting_predictions(text_probas, image_probas, weights_60_40)\n",
        "soft_voting_predictions_70_30 = soft_voting_predictions(text_probas, image_probas, weights_70_30)\n",
        "\n",
        "def calculate_and_print_metrics(true_labels, predictions, model_name):\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    precision = precision_score(true_labels, predictions, average='macro')\n",
        "    recall = recall_score(true_labels, predictions, average='macro')\n",
        "    f1 = f1_score(true_labels, predictions, average='macro')\n",
        "    print(f\"{model_name} - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}\")\n",
        "\n",
        "text_predictions = np.argmax(text_probas, axis=1)\n",
        "image_predictions = np.argmax(image_probas, axis=1)\n",
        "\n",
        "calculate_and_print_metrics(test_labels, text_predictions, \"Text Model\")\n",
        "calculate_and_print_metrics(test_labels, image_predictions, \"Image Model\")\n",
        "calculate_and_print_metrics(test_labels, soft_voting_predictions_50_50, \"Soft Voting 50-50\")\n",
        "calculate_and_print_metrics(test_labels, soft_voting_predictions_60_40, \"Soft Voting 60-40\")\n",
        "calculate_and_print_metrics(test_labels, soft_voting_predictions_70_30, \"Soft Voting 70-30\")"
      ],
      "metadata": {
        "id": "31jt72PYRZwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_predict_proba(train_features, train_labels, test_features):\n",
        "    model = RandomForestClassifier()\n",
        "    model.fit(train_features, train_labels)\n",
        "    probas = model.predict_proba(test_features)\n",
        "    return probas\n",
        "\n",
        "text_probas = train_and_predict_proba(train_text_features_resampled, train_labels_resampled, test_text_features)\n",
        "image_probas = train_and_predict_proba(train_image_features_resampled, train_labels_resampled, test_image_features)\n",
        "\n",
        "def soft_voting_predictions(text_probas, image_probas, weights):\n",
        "    combined_probas = (text_probas * weights[0] + image_probas * weights[1])\n",
        "    final_predictions = np.argmax(combined_probas, axis=1)\n",
        "    return final_predictions\n",
        "\n",
        "weights_50_50 = [0.5, 0.5]\n",
        "weights_60_40 = [0.6, 0.4]\n",
        "weights_70_30 = [0.7, 0.3]\n",
        "\n",
        "soft_voting_predictions_50_50 = soft_voting_predictions(text_probas, image_probas, weights_50_50)\n",
        "soft_voting_predictions_60_40 = soft_voting_predictions(text_probas, image_probas, weights_60_40)\n",
        "soft_voting_predictions_70_30 = soft_voting_predictions(text_probas, image_probas, weights_70_30)\n",
        "\n",
        "def calculate_and_print_metrics(true_labels, predictions, model_name):\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    precision = precision_score(true_labels, predictions, average='macro')\n",
        "    recall = recall_score(true_labels, predictions, average='macro')\n",
        "    f1 = f1_score(true_labels, predictions, average='macro')\n",
        "    print(f\"{model_name} - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}\")\n",
        "\n",
        "text_predictions = np.argmax(text_probas, axis=1)\n",
        "image_predictions = np.argmax(image_probas, axis=1)\n",
        "\n",
        "calculate_and_print_metrics(test_labels, text_predictions, \"Text Model\")\n",
        "calculate_and_print_metrics(test_labels, image_predictions, \"Image Model\")\n",
        "calculate_and_print_metrics(test_labels, soft_voting_predictions_50_50, \"Soft Voting 50-50\")\n",
        "calculate_and_print_metrics(test_labels, soft_voting_predictions_60_40, \"Soft Voting 60-40\")\n",
        "calculate_and_print_metrics(test_labels, soft_voting_predictions_70_30, \"Soft Voting 70-30\")"
      ],
      "metadata": {
        "id": "cDOaaArXSohS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_predict_proba(train_features, train_labels, test_features):\n",
        "    model = DecisionTreeClassifier()\n",
        "    model.fit(train_features, train_labels)\n",
        "    probas = model.predict_proba(test_features)\n",
        "    return probas\n",
        "\n",
        "text_probas = train_and_predict_proba(train_text_features_resampled, train_labels_resampled, test_text_features)\n",
        "image_probas = train_and_predict_proba(train_image_features_resampled, train_labels_resampled, test_image_features)\n",
        "\n",
        "def soft_voting_predictions(text_probas, image_probas, weights):\n",
        "    combined_probas = (text_probas * weights[0] + image_probas * weights[1])\n",
        "    final_predictions = np.argmax(combined_probas, axis=1)\n",
        "    return final_predictions\n",
        "\n",
        "weights_50_50 = [0.5, 0.5]\n",
        "weights_60_40 = [0.6, 0.4]\n",
        "weights_70_30 = [0.7, 0.3]\n",
        "\n",
        "soft_voting_predictions_50_50 = soft_voting_predictions(text_probas, image_probas, weights_50_50)\n",
        "soft_voting_predictions_60_40 = soft_voting_predictions(text_probas, image_probas, weights_60_40)\n",
        "soft_voting_predictions_70_30 = soft_voting_predictions(text_probas, image_probas, weights_70_30)\n",
        "\n",
        "def calculate_and_print_metrics(true_labels, predictions, model_name):\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    precision = precision_score(true_labels, predictions, average='macro')\n",
        "    recall = recall_score(true_labels, predictions, average='macro')\n",
        "    f1 = f1_score(true_labels, predictions, average='macro')\n",
        "    print(f\"{model_name} - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}\")\n",
        "\n",
        "text_predictions = np.argmax(text_probas, axis=1)\n",
        "image_predictions = np.argmax(image_probas, axis=1)\n",
        "\n",
        "calculate_and_print_metrics(test_labels, text_predictions, \"Text Model\")\n",
        "calculate_and_print_metrics(test_labels, image_predictions, \"Image Model\")\n",
        "calculate_and_print_metrics(test_labels, soft_voting_predictions_50_50, \"Soft Voting 50-50\")\n",
        "calculate_and_print_metrics(test_labels, soft_voting_predictions_60_40, \"Soft Voting 60-40\")\n",
        "calculate_and_print_metrics(test_labels, soft_voting_predictions_70_30, \"Soft Voting 70-30\")"
      ],
      "metadata": {
        "id": "eF3oWSkZIG0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_predict_proba(train_features, train_labels, test_features):\n",
        "    model = LogisticRegression()\n",
        "    model.fit(train_features, train_labels)\n",
        "    probas = model.predict_proba(test_features)\n",
        "    return probas\n",
        "\n",
        "text_probas = train_and_predict_proba(train_text_features_resampled, train_labels_resampled, test_text_features)\n",
        "image_probas = train_and_predict_proba(train_image_features_resampled, train_labels_resampled, test_image_features)\n",
        "\n",
        "def soft_voting_predictions(text_probas, image_probas, weights):\n",
        "    combined_probas = (text_probas * weights[0] + image_probas * weights[1])\n",
        "    final_predictions = np.argmax(combined_probas, axis=1)\n",
        "    return final_predictions\n",
        "\n",
        "weights_50_50 = [0.5, 0.5]\n",
        "weights_60_40 = [0.6, 0.4]\n",
        "weights_70_30 = [0.7, 0.3]\n",
        "\n",
        "soft_voting_predictions_50_50 = soft_voting_predictions(text_probas, image_probas, weights_50_50)\n",
        "soft_voting_predictions_60_40 = soft_voting_predictions(text_probas, image_probas, weights_60_40)\n",
        "soft_voting_predictions_70_30 = soft_voting_predictions(text_probas, image_probas, weights_70_30)\n",
        "\n",
        "def calculate_and_print_metrics(true_labels, predictions, model_name):\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    precision = precision_score(true_labels, predictions, average='macro')\n",
        "    recall = recall_score(true_labels, predictions, average='macro')\n",
        "    f1 = f1_score(true_labels, predictions, average='macro')\n",
        "    print(f\"{model_name} - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}\")\n",
        "\n",
        "text_predictions = np.argmax(text_probas, axis=1)\n",
        "image_predictions = np.argmax(image_probas, axis=1)\n",
        "\n",
        "calculate_and_print_metrics(test_labels, text_predictions, \"Text Model\")\n",
        "calculate_and_print_metrics(test_labels, image_predictions, \"Image Model\")\n",
        "calculate_and_print_metrics(test_labels, soft_voting_predictions_50_50, \"Soft Voting 50-50\")\n",
        "calculate_and_print_metrics(test_labels, soft_voting_predictions_60_40, \"Soft Voting 60-40\")\n",
        "calculate_and_print_metrics(test_labels, soft_voting_predictions_70_30, \"Soft Voting 70-30\")"
      ],
      "metadata": {
        "id": "wV62zLdHTv--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_predict_proba(train_features, train_labels, test_features):\n",
        "    model = AdaBoostClassifier()\n",
        "    model.fit(train_features, train_labels)\n",
        "    probas = model.predict_proba(test_features)\n",
        "    return probas\n",
        "\n",
        "text_probas = train_and_predict_proba(train_text_features_resampled, train_labels_resampled, test_text_features)\n",
        "image_probas = train_and_predict_proba(train_image_features_resampled, train_labels_resampled, test_image_features)\n",
        "\n",
        "def soft_voting_predictions(text_probas, image_probas, weights):\n",
        "    combined_probas = (text_probas * weights[0] + image_probas * weights[1])\n",
        "    final_predictions = np.argmax(combined_probas, axis=1)\n",
        "    return final_predictions\n",
        "\n",
        "weights_50_50 = [0.5, 0.5]\n",
        "weights_60_40 = [0.6, 0.4]\n",
        "weights_70_30 = [0.7, 0.3]\n",
        "\n",
        "soft_voting_predictions_50_50 = soft_voting_predictions(text_probas, image_probas, weights_50_50)\n",
        "soft_voting_predictions_60_40 = soft_voting_predictions(text_probas, image_probas, weights_60_40)\n",
        "soft_voting_predictions_70_30 = soft_voting_predictions(text_probas, image_probas, weights_70_30)\n",
        "\n",
        "def calculate_and_print_metrics(true_labels, predictions, model_name):\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    precision = precision_score(true_labels, predictions, average='macro')\n",
        "    recall = recall_score(true_labels, predictions, average='macro')\n",
        "    f1 = f1_score(true_labels, predictions, average='macro')\n",
        "    print(f\"{model_name} - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}\")\n",
        "\n",
        "text_predictions = np.argmax(text_probas, axis=1)\n",
        "image_predictions = np.argmax(image_probas, axis=1)\n",
        "\n",
        "calculate_and_print_metrics(test_labels, text_predictions, \"Text Model\")\n",
        "calculate_and_print_metrics(test_labels, image_predictions, \"Image Model\")\n",
        "calculate_and_print_metrics(test_labels, soft_voting_predictions_50_50, \"Soft Voting 50-50\")\n",
        "calculate_and_print_metrics(test_labels, soft_voting_predictions_60_40, \"Soft Voting 60-40\")\n",
        "calculate_and_print_metrics(test_labels, soft_voting_predictions_70_30, \"Soft Voting 70-30\")"
      ],
      "metadata": {
        "id": "20FKQngoVIIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8RZjPpBhus6"
      },
      "source": [
        "## Deep Learning (RNN+CNN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dU6GdmSPM1zs"
      },
      "outputs": [],
      "source": [
        "import codecs\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Input, Dense, GlobalAveragePooling2D, LSTM, SpatialDropout1D, Embedding, concatenate, GRU, Bidirectional\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications import InceptionV3, ResNet50, EfficientNetV2S, Xception\n",
        "import cv2\n",
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.layers import Input, Embedding, SpatialDropout1D, LSTM, Dense, GlobalAveragePooling2D, concatenate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "df = pd.read_csv('path_to_dataset/multimodal_tweets_dataset.csv')\n",
        "df['anotasi'] = df['anotasi'].map({'not hate': 0, 'hate': 1})\n",
        "\n",
        "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "for train_index, test_index in sss.split(df, df['anotasi']):\n",
        "    df_train = df.iloc[train_index]\n",
        "    df_test = df.iloc[test_index]\n",
        "\n",
        "train_texts, train_labels, train_images = df_train['tweet_text'].tolist(), df_train['anotasi'].tolist(), df_train['images'].tolist()\n",
        "test_texts, test_labels, test_images = df_test['tweet_text'].tolist(), df_test['anotasi'].tolist(), df_test['images'].tolist()\n",
        "\n",
        "def load_and_preprocess_images(image_folder, image_names, target_size):\n",
        "    images = []\n",
        "    for image_name in image_names:\n",
        "        image_path = os.path.join(image_folder, image_name)\n",
        "        image = cv2.imread(image_path)\n",
        "        if image is not None:\n",
        "            image = cv2.resize(image, target_size)\n",
        "            image = image / 255.0\n",
        "            print(image.shape)\n",
        "            images.append(image)\n",
        "    return np.array(images)\n",
        "\n",
        "train_images = load_and_preprocess_images(\"/content/drive/MyDrive/Multimodal Research/selected_images/\", train_images, (224, 224))\n",
        "test_images = load_and_preprocess_images(\"/content/drive/MyDrive/Multimodal Research/selected_images/\", test_images, (224, 224))\n",
        "\n",
        "max_features = 10000\n",
        "max_length = 100\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_features)\n",
        "tokenizer.fit_on_texts(train_texts)\n",
        "\n",
        "train_texts = tokenizer.texts_to_sequences(train_texts)\n",
        "test_texts = tokenizer.texts_to_sequences(test_texts)\n",
        "\n",
        "train_texts = pad_sequences(train_texts, maxlen=max_length)\n",
        "test_texts = pad_sequences(test_texts, maxlen=max_length)\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "train_labels_encoded = label_encoder.fit_transform(train_labels)\n",
        "\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "train_texts_resampled, train_labels_resampled = ros.fit_resample(train_texts, train_labels_encoded)\n",
        "\n",
        "train_images_reshaped = train_images.reshape((train_images.shape[0], -1))\n",
        "train_images_resampled, train_labels_image_resampled = ros.fit_resample(train_images_reshaped, train_labels_encoded)\n",
        "train_images_resampled = train_images_resampled.reshape((-1, 224, 224, 3))\n",
        "\n",
        "train_labels_resampled = to_categorical(train_labels_resampled, num_classes=2)\n",
        "test_labels = to_categorical(test_labels, num_classes=2)"
      ],
      "metadata": {
        "id": "l_OGKwX3DJxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM\n",
        "text_input = Input(shape=(max_length,))\n",
        "embedded_text = Embedding(max_features, 100, input_length=max_length)(text_input)\n",
        "embedded_text = SpatialDropout1D(0.2)(embedded_text)\n",
        "lstm_output = LSTM(20, dropout=0.2, recurrent_dropout=0.2)(embedded_text)\n",
        "text_output = Dense(2, activation='softmax')(lstm_output)\n",
        "\n",
        "model_text = Model(inputs=text_input, outputs=text_output)\n",
        "model_text.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model_text.fit(train_texts_resampled, train_labels_resampled, epochs=3, batch_size=64)\n",
        "\n",
        "test_labels2 = np.argmax(test_labels, axis=1)\n",
        "def calculate_and_print_metrics(true_labels, predictions, model_name):\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    precision = precision_score(true_labels, predictions, average='macro')\n",
        "    recall = recall_score(true_labels, predictions, average='macro')\n",
        "    f1 = f1_score(true_labels, predictions, average='macro')\n",
        "    print(f\"{model_name} - Accuracy: {accuracy:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}, F1-score: {f1:.3f}\")\n",
        "\n",
        "LSTM_probabilities = model_text.predict(test_texts)\n",
        "LSTM_predictions = np.argmax(LSTM_probabilities, axis=1)\n",
        "calculate_and_print_metrics(test_labels2, LSTM_predictions, \"LSTM\")"
      ],
      "metadata": {
        "id": "7XHOnRjrmH1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#BiLSTM\n",
        "text_input = Input(shape=(max_length,))\n",
        "embedded_text = Embedding(max_features, 100, input_length=max_length)(text_input)\n",
        "embedded_text = SpatialDropout1D(0.2)(embedded_text)\n",
        "bilstm_output = Bidirectional(LSTM(20, dropout=0.2, recurrent_dropout=0.2))(embedded_text)\n",
        "text_output = Dense(2, activation='softmax')(bilstm_output)\n",
        "\n",
        "model_text = Model(inputs=text_input, outputs=text_output)\n",
        "model_text.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model_text.fit(train_texts_resampled, train_labels_resampled, epochs=3, batch_size=64)\n",
        "\n",
        "test_labels2 = np.argmax(test_labels, axis=1)\n",
        "def calculate_and_print_metrics(true_labels, predictions, model_name):\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    precision = precision_score(true_labels, predictions, average='macro')\n",
        "    recall = recall_score(true_labels, predictions, average='macro')\n",
        "    f1 = f1_score(true_labels, predictions, average='macro')\n",
        "    print(f\"{model_name} - Accuracy: {accuracy:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}, F1-score: {f1:.3f}\")\n",
        "\n",
        "BiLSTM_probabilities = model_text.predict(test_texts)\n",
        "BiLSTM_predictions = np.argmax(BiLSTM_probabilities, axis=1)\n",
        "calculate_and_print_metrics(test_labels2, BiLSTM_predictions, \"BiLSTM\")"
      ],
      "metadata": {
        "id": "6YAzZhMOnLal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# InceptionV3\n",
        "image_input = Input(shape=(224, 224, 3))\n",
        "base_model = InceptionV3(weights='imagenet', include_top=False, input_tensor=image_input)\n",
        "x = base_model.output\n",
        "image_cnn = GlobalAveragePooling2D()(x)\n",
        "image_output = Dense(2, activation='softmax')(image_cnn)\n",
        "\n",
        "model_image = Model(inputs=image_input, outputs=image_output)\n",
        "model_image.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model_image.fit(train_images_resampled, train_labels_resampled, epochs=10, batch_size=64)\n",
        "\n",
        "InceptionV3_probabilities = model_image.predict(test_images)\n",
        "\n",
        "test_labels2 = np.argmax(test_labels, axis=1)\n",
        "\n",
        "def calculate_and_print_metrics(true_labels, predictions, model_name):\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    precision = precision_score(true_labels, predictions, average='macro')\n",
        "    recall = recall_score(true_labels, predictions, average='macro')\n",
        "    f1 = f1_score(true_labels, predictions, average='macro')\n",
        "    print(f\"{model_name} - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}\")\n",
        "\n",
        "InceptionV3_predictions = np.argmax(InceptionV3_probabilities, axis=1)\n",
        "calculate_and_print_metrics(test_labels2, InceptionV3_predictions, \"InceptionV3\")"
      ],
      "metadata": {
        "id": "mhinM_a0mWrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EfficientNetV2S\n",
        "image_input = Input(shape=(224, 224, 3))\n",
        "base_model = EfficientNetV2S(weights='imagenet', include_top=False, input_tensor=image_input)\n",
        "x = base_model.output\n",
        "image_cnn = GlobalAveragePooling2D()(x)\n",
        "image_output = Dense(2, activation='softmax')(image_cnn)\n",
        "\n",
        "model_image = Model(inputs=image_input, outputs=image_output)\n",
        "model_image.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model_image.fit(train_images_resampled, train_labels_resampled, epochs=10, batch_size=64)\n",
        "EfficientNetV2S_probabilities = model_image.predict(test_images)\n",
        "test_labels2 = np.argmax(test_labels, axis=1)\n",
        "\n",
        "def calculate_and_print_metrics(true_labels, predictions, model_name):\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    precision = precision_score(true_labels, predictions, average='macro')\n",
        "    recall = recall_score(true_labels, predictions, average='macro')\n",
        "    f1 = f1_score(true_labels, predictions, average='macro')\n",
        "    print(f\"{model_name} - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}\")\n",
        "\n",
        "EfficientNetV2S_predictions = np.argmax(EfficientNetV2S_probabilities, axis=1)\n",
        "calculate_and_print_metrics(test_labels2, EfficientNetV2S_predictions, \"EfficientNetV2S\")"
      ],
      "metadata": {
        "id": "N8yWhkEKmXyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Xception\n",
        "image_input = Input(shape=(224, 224, 3))\n",
        "base_model = Xception(weights='imagenet', include_top=False, input_tensor=image_input)\n",
        "x = base_model.output\n",
        "image_cnn = GlobalAveragePooling2D()(x)\n",
        "image_output = Dense(2, activation='softmax')(image_cnn)\n",
        "\n",
        "model_image = Model(inputs=image_input, outputs=image_output)\n",
        "model_image.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model_image.fit(train_images_resampled, train_labels_resampled, epochs=10, batch_size=64)\n",
        "Xception_probabilities = model_image.predict(test_images)\n",
        "test_labels2 = np.argmax(test_labels, axis=1)\n",
        "\n",
        "def calculate_and_print_metrics(true_labels, predictions, model_name):\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    precision = precision_score(true_labels, predictions, average='macro')\n",
        "    recall = recall_score(true_labels, predictions, average='macro')\n",
        "    f1 = f1_score(true_labels, predictions, average='macro')\n",
        "    print(f\"{model_name} - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}\")\n",
        "\n",
        "Xception_predictions = np.argmax(Xception_probabilities, axis=1)\n",
        "calculate_and_print_metrics(test_labels2, Xception_predictions, \"Xception\")"
      ],
      "metadata": {
        "id": "OiakHmW3nGhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM+InceptionV3\n",
        "def soft_voting_predictions(text_probas, image_probas, weights):\n",
        "    combined_probas = (text_probas * weights[0] + image_probas * weights[1])\n",
        "    final_predictions = np.argmax(combined_probas, axis=1)\n",
        "    return final_predictions\n",
        "\n",
        "text_probas = LSTM_probabilities\n",
        "image_probas = InceptionV3_probabilities\n",
        "\n",
        "weights_50_50 = [0.5, 0.5]\n",
        "weights_60_40 = [0.6, 0.4]\n",
        "weights_70_30 = [0.7, 0.3]\n",
        "\n",
        "soft_voting_predictions_50_50 = soft_voting_predictions(text_probas, image_probas, weights_50_50)\n",
        "soft_voting_predictions_60_40 = soft_voting_predictions(text_probas, image_probas, weights_60_40)\n",
        "soft_voting_predictions_70_30 = soft_voting_predictions(text_probas, image_probas, weights_70_30)\n",
        "\n",
        "test_labels2 = np.argmax(test_labels, axis=1)\n",
        "def calculate_and_print_metrics(true_labels, predictions, model_name):\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    precision = precision_score(true_labels, predictions, average='macro')\n",
        "    recall = recall_score(true_labels, predictions, average='macro')\n",
        "    f1 = f1_score(true_labels, predictions, average='macro')\n",
        "    print(f\"{model_name} - Accuracy: {accuracy:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}, F1-score: {f1:.3f}\")\n",
        "\n",
        "print(\"LSTM+InceptionV3\")\n",
        "calculate_and_print_metrics(test_labels2, soft_voting_predictions_50_50, \"Soft Voting 50-50\")\n",
        "calculate_and_print_metrics(test_labels2, soft_voting_predictions_60_40, \"Soft Voting 60-40\")\n",
        "calculate_and_print_metrics(test_labels2, soft_voting_predictions_70_30, \"Soft Voting 70-30\")"
      ],
      "metadata": {
        "id": "kArQxic_oa7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM+EfficientNetV2S\n",
        "def soft_voting_predictions(text_probas, image_probas, weights):\n",
        "    combined_probas = (text_probas * weights[0] + image_probas * weights[1])\n",
        "    final_predictions = np.argmax(combined_probas, axis=1)\n",
        "    return final_predictions\n",
        "\n",
        "text_probas = LSTM_probabilities\n",
        "image_probas = EfficientNetV2S_probabilities\n",
        "\n",
        "weights_50_50 = [0.5, 0.5]\n",
        "weights_60_40 = [0.6, 0.4]\n",
        "weights_70_30 = [0.7, 0.3]\n",
        "\n",
        "soft_voting_predictions_50_50 = soft_voting_predictions(text_probas, image_probas, weights_50_50)\n",
        "soft_voting_predictions_60_40 = soft_voting_predictions(text_probas, image_probas, weights_60_40)\n",
        "soft_voting_predictions_70_30 = soft_voting_predictions(text_probas, image_probas, weights_70_30)\n",
        "\n",
        "test_labels2 = np.argmax(test_labels, axis=1)\n",
        "def calculate_and_print_metrics(true_labels, predictions, model_name):\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    precision = precision_score(true_labels, predictions, average='macro')\n",
        "    recall = recall_score(true_labels, predictions, average='macro')\n",
        "    f1 = f1_score(true_labels, predictions, average='macro')\n",
        "    print(f\"{model_name} - Accuracy: {accuracy:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}, F1-score: {f1:.3f}\")\n",
        "\n",
        "print(\"LSTM+EfficientNetV2S\")\n",
        "calculate_and_print_metrics(test_labels2, soft_voting_predictions_50_50, \"Soft Voting 50-50\")\n",
        "calculate_and_print_metrics(test_labels2, soft_voting_predictions_60_40, \"Soft Voting 60-40\")\n",
        "calculate_and_print_metrics(test_labels2, soft_voting_predictions_70_30, \"Soft Voting 70-30\")"
      ],
      "metadata": {
        "id": "al6B5emvo8Ra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM+Xception\n",
        "def soft_voting_predictions(text_probas, image_probas, weights):\n",
        "    combined_probas = (text_probas * weights[0] + image_probas * weights[1])\n",
        "    final_predictions = np.argmax(combined_probas, axis=1)\n",
        "    return final_predictions\n",
        "\n",
        "text_probas = LSTM_probabilities\n",
        "image_probas = Xception_probabilities\n",
        "\n",
        "weights_50_50 = [0.5, 0.5]\n",
        "weights_60_40 = [0.6, 0.4]\n",
        "weights_70_30 = [0.7, 0.3]\n",
        "\n",
        "soft_voting_predictions_50_50 = soft_voting_predictions(text_probas, image_probas, weights_50_50)\n",
        "soft_voting_predictions_60_40 = soft_voting_predictions(text_probas, image_probas, weights_60_40)\n",
        "soft_voting_predictions_70_30 = soft_voting_predictions(text_probas, image_probas, weights_70_30)\n",
        "\n",
        "test_labels2 = np.argmax(test_labels, axis=1)\n",
        "def calculate_and_print_metrics(true_labels, predictions, model_name):\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    precision = precision_score(true_labels, predictions, average='macro')\n",
        "    recall = recall_score(true_labels, predictions, average='macro')\n",
        "    f1 = f1_score(true_labels, predictions, average='macro')\n",
        "    print(f\"{model_name} - Accuracy: {accuracy:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}, F1-score: {f1:.3f}\")\n",
        "\n",
        "print(\"LSTM+Xception\")\n",
        "calculate_and_print_metrics(test_labels2, soft_voting_predictions_50_50, \"Soft Voting 50-50\")\n",
        "calculate_and_print_metrics(test_labels2, soft_voting_predictions_60_40, \"Soft Voting 60-40\")\n",
        "calculate_and_print_metrics(test_labels2, soft_voting_predictions_70_30, \"Soft Voting 70-30\")"
      ],
      "metadata": {
        "id": "P5_q3-IxpWaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BiLSTM+InceptionV3\n",
        "def soft_voting_predictions(text_probas, image_probas, weights):\n",
        "    combined_probas = (text_probas * weights[0] + image_probas * weights[1])\n",
        "    final_predictions = np.argmax(combined_probas, axis=1)\n",
        "    return final_predictions\n",
        "\n",
        "text_probas = BiLSTM_probabilities\n",
        "image_probas = InceptionV3_probabilities\n",
        "\n",
        "weights_50_50 = [0.5, 0.5]\n",
        "weights_60_40 = [0.6, 0.4]\n",
        "weights_70_30 = [0.7, 0.3]\n",
        "\n",
        "soft_voting_predictions_50_50 = soft_voting_predictions(text_probas, image_probas, weights_50_50)\n",
        "soft_voting_predictions_60_40 = soft_voting_predictions(text_probas, image_probas, weights_60_40)\n",
        "soft_voting_predictions_70_30 = soft_voting_predictions(text_probas, image_probas, weights_70_30)\n",
        "\n",
        "test_labels2 = np.argmax(test_labels, axis=1)\n",
        "def calculate_and_print_metrics(true_labels, predictions, model_name):\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    precision = precision_score(true_labels, predictions, average='macro')\n",
        "    recall = recall_score(true_labels, predictions, average='macro')\n",
        "    f1 = f1_score(true_labels, predictions, average='macro')\n",
        "    print(f\"{model_name} - Accuracy: {accuracy:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}, F1-score: {f1:.3f}\")\n",
        "\n",
        "print(\"BiLSTM+InceptionV3\")\n",
        "calculate_and_print_metrics(test_labels2, soft_voting_predictions_50_50, \"Soft Voting 50-50\")\n",
        "calculate_and_print_metrics(test_labels2, soft_voting_predictions_60_40, \"Soft Voting 60-40\")\n",
        "calculate_and_print_metrics(test_labels2, soft_voting_predictions_70_30, \"Soft Voting 70-30\")"
      ],
      "metadata": {
        "id": "CVAAgk7tpj_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BiLSTM+EfficientNetV2S\n",
        "def soft_voting_predictions(text_probas, image_probas, weights):\n",
        "    combined_probas = (text_probas * weights[0] + image_probas * weights[1])\n",
        "    final_predictions = np.argmax(combined_probas, axis=1)\n",
        "    return final_predictions\n",
        "\n",
        "text_probas = BiLSTM_probabilities\n",
        "image_probas = EfficientNetV2S_probabilities\n",
        "\n",
        "weights_50_50 = [0.5, 0.5]\n",
        "weights_60_40 = [0.6, 0.4]\n",
        "weights_70_30 = [0.7, 0.3]\n",
        "\n",
        "soft_voting_predictions_50_50 = soft_voting_predictions(text_probas, image_probas, weights_50_50)\n",
        "soft_voting_predictions_60_40 = soft_voting_predictions(text_probas, image_probas, weights_60_40)\n",
        "soft_voting_predictions_70_30 = soft_voting_predictions(text_probas, image_probas, weights_70_30)\n",
        "\n",
        "test_labels2 = np.argmax(test_labels, axis=1)\n",
        "def calculate_and_print_metrics(true_labels, predictions, model_name):\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    precision = precision_score(true_labels, predictions, average='macro')\n",
        "    recall = recall_score(true_labels, predictions, average='macro')\n",
        "    f1 = f1_score(true_labels, predictions, average='macro')\n",
        "    print(f\"{model_name} - Accuracy: {accuracy:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}, F1-score: {f1:.3f}\")\n",
        "\n",
        "print(\"BiLSTM+EfficientNetV2S\")\n",
        "calculate_and_print_metrics(test_labels2, soft_voting_predictions_50_50, \"Soft Voting 50-50\")\n",
        "calculate_and_print_metrics(test_labels2, soft_voting_predictions_60_40, \"Soft Voting 60-40\")\n",
        "calculate_and_print_metrics(test_labels2, soft_voting_predictions_70_30, \"Soft Voting 70-30\")"
      ],
      "metadata": {
        "id": "egtaFFXLpzGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BiLSTM+Xception\n",
        "def soft_voting_predictions(text_probas, image_probas, weights):\n",
        "    combined_probas = (text_probas * weights[0] + image_probas * weights[1])\n",
        "    final_predictions = np.argmax(combined_probas, axis=1)\n",
        "    return final_predictions\n",
        "\n",
        "text_probas = BiLSTM_probabilities\n",
        "image_probas = Xception_probabilities\n",
        "\n",
        "weights_50_50 = [0.5, 0.5]\n",
        "weights_60_40 = [0.6, 0.4]\n",
        "weights_70_30 = [0.7, 0.3]\n",
        "\n",
        "soft_voting_predictions_50_50 = soft_voting_predictions(text_probas, image_probas, weights_50_50)\n",
        "soft_voting_predictions_60_40 = soft_voting_predictions(text_probas, image_probas, weights_60_40)\n",
        "soft_voting_predictions_70_30 = soft_voting_predictions(text_probas, image_probas, weights_70_30)\n",
        "\n",
        "test_labels2 = np.argmax(test_labels, axis=1)\n",
        "def calculate_and_print_metrics(true_labels, predictions, model_name):\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    precision = precision_score(true_labels, predictions, average='macro')\n",
        "    recall = recall_score(true_labels, predictions, average='macro')\n",
        "    f1 = f1_score(true_labels, predictions, average='macro')\n",
        "    print(f\"{model_name} - Accuracy: {accuracy:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}, F1-score: {f1:.3f}\")\n",
        "\n",
        "print(\"BiLSTM+Xception\")\n",
        "calculate_and_print_metrics(test_labels2, soft_voting_predictions_50_50, \"Soft Voting 50-50\")\n",
        "calculate_and_print_metrics(test_labels2, soft_voting_predictions_60_40, \"Soft Voting 60-40\")\n",
        "calculate_and_print_metrics(test_labels2, soft_voting_predictions_70_30, \"Soft Voting 70-30\")"
      ],
      "metadata": {
        "id": "71pZ1oYeqBdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BERT+CNN"
      ],
      "metadata": {
        "id": "pl3DrIdZhSAH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification\n",
        "from torch.nn.functional import softmax\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/Multimodal Indonesia Dataset/multimodal_tweets_dataset.csv')\n",
        "df['anotasi'] = df['anotasi'].map({'not hate': 0, 'hate': 1})\n",
        "\n",
        "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "for train_index, test_index in sss.split(df, df['anotasi']):\n",
        "    df_train = df.iloc[train_index]\n",
        "    df_test = df.iloc[test_index]\n",
        "\n",
        "train_texts, train_labels, train_images = df_train['tweet_text'].tolist(), df_train['anotasi'].tolist(), df_train['images'].tolist()\n",
        "test_texts, test_labels, test_images = df_test['tweet_text'].tolist(), df_test['anotasi'].tolist(), df_test['images'].tolist()\n",
        "\n",
        "def load_and_preprocess_images(image_folder, image_names, target_size):\n",
        "    images = []\n",
        "    for image_name in image_names:\n",
        "        image_path = os.path.join(image_folder, image_name)\n",
        "        image = cv2.imread(image_path)\n",
        "        if image is not None:\n",
        "            image = cv2.resize(image, target_size)\n",
        "            image = image / 255.0\n",
        "            print(image.shape)\n",
        "            images.append(image)\n",
        "    return np.array(images)\n",
        "\n",
        "train_images = load_and_preprocess_images(\"/content/drive/MyDrive/Multimodal Research/selected_images/\", train_images, (224, 224))\n",
        "test_images = load_and_preprocess_images(\"/content/drive/MyDrive/Multimodal Research/selected_images/\", test_images, (224, 224))\n",
        "\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "train_texts, train_labels = ros.fit_resample(np.array(train_texts).reshape(-1, 1), train_labels)\n",
        "train_texts = train_texts.flatten()\n",
        "\n",
        "train_images_reshaped = train_images.reshape((train_images.shape[0], -1))\n",
        "train_images_resampled, train_labels_image_resampled = ros.fit_resample(train_images_reshaped, train_labels_encoded)\n",
        "train_images_resampled = train_images_resampled.reshape((-1, 224, 224, 3))"
      ],
      "metadata": {
        "id": "98BYZpbIXBtk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"indolem/indobert-base-uncased\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"indolem/indobert-base-uncased\", num_labels=2)\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        labels = self.labels[idx]\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_length,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(labels, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "train_dataset = TextDataset(train_texts, train_labels, tokenizer)\n",
        "test_dataset = TextDataset(test_texts, test_labels, tokenizer)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=4)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "\n",
        "for epoch in range(3):  # Number of epochs\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(train_loader)}\")\n",
        "\n",
        "model.eval()\n",
        "IndoBERT_predictions = []\n",
        "all_labels = []\n",
        "IndoBERT_probas = []\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "        probas = softmax(logits, dim=-1)\n",
        "        IndoBERT_probas.extend(probas.cpu().numpy())\n",
        "\n",
        "        predictions = torch.argmax(logits, dim=-1)\n",
        "        IndoBERT_predictions.extend(predictions.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "accuracy = accuracy_score(all_labels, IndoBERT_predictions)\n",
        "precision = precision_score(all_labels, IndoBERT_predictions, average='macro')\n",
        "recall = recall_score(all_labels, IndoBERT_predictions, average='macro')\n",
        "f1 = f1_score(all_labels, IndoBERT_predictions, average='macro')\n",
        "\n",
        "print(\"IndoBERT - Accuracy: \", accuracy)\n",
        "print(\"IndoBERT - Precision: \", precision)\n",
        "print(\"IndoBERT - Recall: \", recall)\n",
        "print(\"IndoBERT - F1: \", f1)"
      ],
      "metadata": {
        "id": "gJEFLRjkXu2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = DistilBertTokenizer.from_pretrained('cahya/distilbert-base-indonesian')\n",
        "model = DistilBertForSequenceClassification.from_pretrained('cahya/distilbert-base-indonesian', num_labels=2)\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        labels = self.labels[idx]\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_length,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(labels, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "train_dataset = TextDataset(train_texts, train_labels, tokenizer)\n",
        "test_dataset = TextDataset(test_texts, test_labels, tokenizer)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=4)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "\n",
        "for epoch in range(3):  # Number of epochs\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(train_loader)}\")\n",
        "\n",
        "model.eval()\n",
        "DistilBert_predictions = []\n",
        "all_labels = []\n",
        "DistilBert_probas = []\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "        probas = softmax(logits, dim=-1)\n",
        "        DistilBert_probas.extend(probas.cpu().numpy())\n",
        "\n",
        "        predictions = torch.argmax(logits, dim=-1)\n",
        "        DistilBert_predictions.extend(predictions.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "accuracy = accuracy_score(all_labels, DistilBert_predictions)\n",
        "precision = precision_score(all_labels, DistilBert_predictions, average='macro')\n",
        "recall = recall_score(all_labels, DistilBert_predictions, average='macro')\n",
        "f1 = f1_score(all_labels, DistilBert_predictions, average='macro')\n",
        "\n",
        "print(\"DistilBert - Accuracy: \", accuracy)\n",
        "print(\"DistilBert - Precision: \", precision)\n",
        "print(\"DistilBert - Recall: \", recall)\n",
        "print(\"DistilBert - F1: \", f1)"
      ],
      "metadata": {
        "id": "ecHLBF-raCIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n",
        "model = XLMRobertaForSequenceClassification.from_pretrained('xlm-roberta-base', num_labels=2)\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        labels = self.labels[idx]\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_length,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(labels, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "train_dataset = TextDataset(train_texts, train_labels, tokenizer)\n",
        "test_dataset = TextDataset(test_texts, test_labels, tokenizer)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=4)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "\n",
        "for epoch in range(3):  # Number of epochs\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(train_loader)}\")\n",
        "\n",
        "model.eval()\n",
        "XLMRoberta_predictions = []\n",
        "all_labels = []\n",
        "XLMRoberta_probas = []\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "        probas = softmax(logits, dim=-1)\n",
        "        XLMRoberta_probas.extend(probas.cpu().numpy())\n",
        "\n",
        "        predictions = torch.argmax(logits, dim=-1)\n",
        "        XLMRoberta_predictions.extend(predictions.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "accuracy = accuracy_score(all_labels, XLMRoberta_predictions)\n",
        "precision = precision_score(all_labels, XLMRoberta_predictions, average='macro')\n",
        "recall = recall_score(all_labels, XLMRoberta_predictions, average='macro')\n",
        "f1 = f1_score(all_labels, XLMRoberta_predictions, average='macro')\n",
        "\n",
        "print(\"XLMRoberta - Accuracy: \", accuracy)\n",
        "print(\"XLMRoberta - Precision: \", precision)\n",
        "print(\"XLMRoberta - Recall: \", recall)\n",
        "print(\"XLMRoberta - F1: \", f1)"
      ],
      "metadata": {
        "id": "iaOSkFUfbDEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# InceptionV3\n",
        "image_input = Input(shape=(224, 224, 3))\n",
        "base_model = InceptionV3(weights='imagenet', include_top=False, input_tensor=image_input)\n",
        "x = base_model.output\n",
        "image_cnn = GlobalAveragePooling2D()(x)\n",
        "image_output = Dense(2, activation='softmax')(image_cnn)\n",
        "\n",
        "model_image = Model(inputs=image_input, outputs=image_output)\n",
        "model_image.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model_image.fit(train_images_resampled, train_labels_resampled, epochs=10, batch_size=64)\n",
        "\n",
        "InceptionV3_probabilities = model_image.predict(test_images)\n",
        "\n",
        "test_labels2 = np.argmax(test_labels, axis=1)\n",
        "\n",
        "def calculate_and_print_metrics(true_labels, predictions, model_name):\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    precision = precision_score(true_labels, predictions, average='macro')\n",
        "    recall = recall_score(true_labels, predictions, average='macro')\n",
        "    f1 = f1_score(true_labels, predictions, average='macro')\n",
        "    print(f\"{model_name} - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}\")\n",
        "\n",
        "InceptionV3_predictions = np.argmax(InceptionV3_probabilities, axis=1)\n",
        "calculate_and_print_metrics(test_labels2, InceptionV3_predictions, \"InceptionV3\")"
      ],
      "metadata": {
        "id": "yQL8WqXGSyi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EfficientNetV2S\n",
        "image_input = Input(shape=(224, 224, 3))\n",
        "base_model = EfficientNetV2S(weights='imagenet', include_top=False, input_tensor=image_input)\n",
        "x = base_model.output\n",
        "image_cnn = GlobalAveragePooling2D()(x)\n",
        "image_output = Dense(2, activation='softmax')(image_cnn)\n",
        "\n",
        "model_image = Model(inputs=image_input, outputs=image_output)\n",
        "model_image.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model_image.fit(train_images_resampled, train_labels_resampled, epochs=10, batch_size=64)\n",
        "EfficientNetV2S_probabilities = model_image.predict(test_images)\n",
        "test_labels2 = np.argmax(test_labels, axis=1)\n",
        "\n",
        "def calculate_and_print_metrics(true_labels, predictions, model_name):\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    precision = precision_score(true_labels, predictions, average='macro')\n",
        "    recall = recall_score(true_labels, predictions, average='macro')\n",
        "    f1 = f1_score(true_labels, predictions, average='macro')\n",
        "    print(f\"{model_name} - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}\")\n",
        "\n",
        "EfficientNetV2S_predictions = np.argmax(EfficientNetV2S_probabilities, axis=1)\n",
        "calculate_and_print_metrics(test_labels2, EfficientNetV2S_predictions, \"EfficientNetV2S\")"
      ],
      "metadata": {
        "id": "z8TXCJ4IT2IF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Xception\n",
        "image_input = Input(shape=(224, 224, 3))\n",
        "base_model = Xception(weights='imagenet', include_top=False, input_tensor=image_input)\n",
        "x = base_model.output\n",
        "image_cnn = GlobalAveragePooling2D()(x)\n",
        "image_output = Dense(2, activation='softmax')(image_cnn)\n",
        "\n",
        "model_image = Model(inputs=image_input, outputs=image_output)\n",
        "model_image.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model_image.fit(train_images_resampled, train_labels_resampled, epochs=10, batch_size=64)\n",
        "Xception_probabilities = model_image.predict(test_images)\n",
        "test_labels2 = np.argmax(test_labels, axis=1)\n",
        "\n",
        "def calculate_and_print_metrics(true_labels, predictions, model_name):\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    precision = precision_score(true_labels, predictions, average='macro')\n",
        "    recall = recall_score(true_labels, predictions, average='macro')\n",
        "    f1 = f1_score(true_labels, predictions, average='macro')\n",
        "    print(f\"{model_name} - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}\")\n",
        "\n",
        "Xception_predictions = np.argmax(Xception_probabilities, axis=1)\n",
        "calculate_and_print_metrics(test_labels2, Xception_predictions, \"Xception\")"
      ],
      "metadata": {
        "id": "c2E81qkdUkg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# IndoBERT+InceptionV3\n",
        "def soft_voting_predictions(text_probas, image_probas, weights):\n",
        "    combined_probas = (text_probas * weights[0] + image_probas * weights[1])\n",
        "    final_predictions = np.argmax(combined_probas, axis=1)\n",
        "    return final_predictions\n",
        "\n",
        "text_probas = np.array(IndoBERT_probas)\n",
        "image_probas = InceptionV3_probabilities\n",
        "\n",
        "weights_50_50 = [0.5, 0.5]\n",
        "weights_60_40 = [0.6, 0.4]\n",
        "weights_70_30 = [0.7, 0.3]\n",
        "\n",
        "soft_voting_predictions_50_50 = soft_voting_predictions(text_probas, image_probas, weights_50_50)\n",
        "soft_voting_predictions_60_40 = soft_voting_predictions(text_probas, image_probas, weights_60_40)\n",
        "soft_voting_predictions_70_30 = soft_voting_predictions(text_probas, image_probas, weights_70_30)\n",
        "\n",
        "test_labels2 = np.argmax(test_labels, axis=1)\n",
        "def calculate_and_print_metrics(true_labels, predictions, model_name):\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    precision = precision_score(true_labels, predictions, average='macro')\n",
        "    recall = recall_score(true_labels, predictions, average='macro')\n",
        "    f1 = f1_score(true_labels, predictions, average='macro')\n",
        "    print(f\"{model_name} - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}\")\n",
        "\n",
        "print(\"IndoBERT+InceptionV3\")\n",
        "calculate_and_print_metrics(test_labels2, soft_voting_predictions_50_50, \"Soft Voting 50-50\")\n",
        "calculate_and_print_metrics(test_labels2, soft_voting_predictions_60_40, \"Soft Voting 60-40\")\n",
        "calculate_and_print_metrics(test_labels2, soft_voting_predictions_70_30, \"Soft Voting 70-30\")"
      ],
      "metadata": {
        "id": "3rf4x7hBWBSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# IndoBERT+EfficientNetV2S\n",
        "def soft_voting_predictions(text_probas, image_probas, weights):\n",
        "    combined_probas = (text_probas * weights[0] + image_probas * weights[1])\n",
        "    final_predictions = np.argmax(combined_probas, axis=1)\n",
        "    return final_predictions\n",
        "\n",
        "text_probas = np.array(IndoBERT_probas)\n",
        "image_probas = EfficientNetV2S_probabilities\n",
        "\n",
        "weights_50_50 = [0.5, 0.5]\n",
        "weights_60_40 = [0.6, 0.4]\n",
        "weights_70_30 = [0.7, 0.3]\n",
        "\n",
        "soft_voting_predictions_50_50 = soft_voting_predictions(text_probas, image_probas, weights_50_50)\n",
        "soft_voting_predictions_60_40 = soft_voting_predictions(text_probas, image_probas, weights_60_40)\n",
        "soft_voting_predictions_70_30 = soft_voting_predictions(text_probas, image_probas, weights_70_30)\n",
        "\n",
        "test_labels2 = np.argmax(test_labels, axis=1)\n",
        "def calculate_and_print_metrics(true_labels, predictions, model_name):\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    precision = precision_score(true_labels, predictions, average='macro')\n",
        "    recall = recall_score(true_labels, predictions, average='macro')\n",
        "    f1 = f1_score(true_labels, predictions, average='macro')\n",
        "    print(f\"{model_name} - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}\")\n",
        "\n",
        "print(\"IndoBERT+EfficientNetV2S\")\n",
        "calculate_and_print_metrics(test_labels2, soft_voting_predictions_50_50, \"Soft Voting 50-50\")\n",
        "calculate_and_print_metrics(test_labels2, soft_voting_predictions_60_40, \"Soft Voting 60-40\")\n",
        "calculate_and_print_metrics(test_labels2, soft_voting_predictions_70_30, \"Soft Voting 70-30\")"
      ],
      "metadata": {
        "id": "0qo1ZRCzixAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# IndoBERT+Xception\n",
        "def soft_voting_predictions(text_probas, image_probas, weights):\n",
        "    combined_probas = (text_probas * weights[0] + image_probas * weights[1])\n",
        "    final_predictions = np.argmax(combined_probas, axis=1)\n",
        "    return final_predictions\n",
        "\n",
        "text_probas = np.array(IndoBERT_probas)\n",
        "image_probas = Xception_probabilities\n",
        "\n",
        "weights_50_50 = [0.5, 0.5]\n",
        "weights_60_40 = [0.6, 0.4]\n",
        "weights_70_30 = [0.7, 0.3]\n",
        "\n",
        "soft_voting_predictions_50_50 = soft_voting_predictions(text_probas, image_probas, weights_50_50)\n",
        "soft_voting_predictions_60_40 = soft_voting_predictions(text_probas, image_probas, weights_60_40)\n",
        "soft_voting_predictions_70_30 = soft_voting_predictions(text_probas, image_probas, weights_70_30)\n",
        "\n",
        "test_labels2 = np.argmax(test_labels, axis=1)\n",
        "def calculate_and_print_metrics(true_labels, predictions, model_name):\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    precision = precision_score(true_labels, predictions, average='macro')\n",
        "    recall = recall_score(true_labels, predictions, average='macro')\n",
        "    f1 = f1_score(true_labels, predictions, average='macro')\n",
        "    print(f\"{model_name} - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}\")\n",
        "\n",
        "print(\"IndoBERT+Xception\")\n",
        "calculate_and_print_metrics(test_labels2, soft_voting_predictions_50_50, \"Soft Voting 50-50\")\n",
        "calculate_and_print_metrics(test_labels2, soft_voting_predictions_60_40, \"Soft Voting 60-40\")\n",
        "calculate_and_print_metrics(test_labels2, soft_voting_predictions_70_30, \"Soft Voting 70-30\")"
      ],
      "metadata": {
        "id": "ckEwqnHPjEhr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DistilBert+InceptionV3\n",
        "def soft_voting_predictions(text_probas, image_probas, weights):\n",
        "    combined_probas = (text_probas * weights[0] + image_probas * weights[1])\n",
        "    final_predictions = np.argmax(combined_probas, axis=1)\n",
        "    return final_predictions\n",
        "\n",
        "text_probas = np.array(DistilBert_probas)\n",
        "image_probas = InceptionV3_probabilities\n",
        "\n",
        "weights_50_50 = [0.5, 0.5]\n",
        "weights_60_40 = [0.6, 0.4]\n",
        "weights_70_30 = [0.7, 0.3]\n",
        "\n",
        "soft_voting_predictions_50_50 = soft_voting_predictions(text_probas, image_probas, weights_50_50)\n",
        "soft_voting_predictions_60_40 = soft_voting_predictions(text_probas, image_probas, weights_60_40)\n",
        "soft_voting_predictions_70_30 = soft_voting_predictions(text_probas, image_probas, weights_70_30)\n",
        "\n",
        "test_labels2 = np.argmax(test_labels, axis=1)\n",
        "def calculate_and_print_metrics(true_labels, predictions, model_name):\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    precision = precision_score(true_labels, predictions, average='macro')\n",
        "    recall = recall_score(true_labels, predictions, average='macro')\n",
        "    f1 = f1_score(true_labels, predictions, average='macro')\n",
        "    print(f\"{model_name} - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}\")\n",
        "\n",
        "print(\"DistilBert+InceptionV3\")\n",
        "calculate_and_print_metrics(test_labels2, soft_voting_predictions_50_50, \"Soft Voting 50-50\")\n",
        "calculate_and_print_metrics(test_labels2, soft_voting_predictions_60_40, \"Soft Voting 60-40\")\n",
        "calculate_and_print_metrics(test_labels2, soft_voting_predictions_70_30, \"Soft Voting 70-30\")"
      ],
      "metadata": {
        "id": "7Z3WLfs6jXpD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DistilBert+EfficientNetV2S\n",
        "def soft_voting_predictions(text_probas, image_probas, weights):\n",
        "    combined_probas = (text_probas * weights[0] + image_probas * weights[1])\n",
        "    final_predictions = np.argmax(combined_probas, axis=1)\n",
        "    return final_predictions\n",
        "\n",
        "text_probas = np.array(DistilBert_probas)\n",
        "image_probas = EfficientNetV2S_probabilities\n",
        "\n",
        "weights_50_50 = [0.5, 0.5]\n",
        "weights_60_40 = [0.6, 0.4]\n",
        "weights_70_30 = [0.7, 0.3]\n",
        "\n",
        "soft_voting_predictions_50_50 = soft_voting_predictions(text_probas, image_probas, weights_50_50)\n",
        "soft_voting_predictions_60_40 = soft_voting_predictions(text_probas, image_probas, weights_60_40)\n",
        "soft_voting_predictions_70_30 = soft_voting_predictions(text_probas, image_probas, weights_70_30)\n",
        "\n",
        "test_labels2 = np.argmax(test_labels, axis=1)\n",
        "def calculate_and_print_metrics(true_labels, predictions, model_name):\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    precision = precision_score(true_labels, predictions, average='macro')\n",
        "    recall = recall_score(true_labels, predictions, average='macro')\n",
        "    f1 = f1_score(true_labels, predictions, average='macro')\n",
        "    print(f\"{model_name} - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}\")\n",
        "\n",
        "print(\"DistilBert+EfficientNetV2S\")\n",
        "calculate_and_print_metrics(test_labels2, soft_voting_predictions_50_50, \"Soft Voting 50-50\")\n",
        "calculate_and_print_metrics(test_labels2, soft_voting_predictions_60_40, \"Soft Voting 60-40\")\n",
        "calculate_and_print_metrics(test_labels2, soft_voting_predictions_70_30, \"Soft Voting 70-30\")"
      ],
      "metadata": {
        "id": "pupJ2XoSjx4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DistilBert+Xception\n",
        "def soft_voting_predictions(text_probas, image_probas, weights):\n",
        "    combined_probas = (text_probas * weights[0] + image_probas * weights[1])\n",
        "    final_predictions = np.argmax(combined_probas, axis=1)\n",
        "    return final_predictions\n",
        "\n",
        "text_probas = np.array(DistilBert_probas)\n",
        "image_probas = Xception_probabilities\n",
        "\n",
        "weights_50_50 = [0.5, 0.5]\n",
        "weights_60_40 = [0.6, 0.4]\n",
        "weights_70_30 = [0.7, 0.3]\n",
        "\n",
        "soft_voting_predictions_50_50 = soft_voting_predictions(text_probas, image_probas, weights_50_50)\n",
        "soft_voting_predictions_60_40 = soft_voting_predictions(text_probas, image_probas, weights_60_40)\n",
        "soft_voting_predictions_70_30 = soft_voting_predictions(text_probas, image_probas, weights_70_30)\n",
        "\n",
        "test_labels2 = np.argmax(test_labels, axis=1)\n",
        "def calculate_and_print_metrics(true_labels, predictions, model_name):\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    precision = precision_score(true_labels, predictions, average='macro')\n",
        "    recall = recall_score(true_labels, predictions, average='macro')\n",
        "    f1 = f1_score(true_labels, predictions, average='macro')\n",
        "    print(f\"{model_name} - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}\")\n",
        "\n",
        "print(\"DistilBert+Xception\")\n",
        "calculate_and_print_metrics(test_labels2, soft_voting_predictions_50_50, \"Soft Voting 50-50\")\n",
        "calculate_and_print_metrics(test_labels2, soft_voting_predictions_60_40, \"Soft Voting 60-40\")\n",
        "calculate_and_print_metrics(test_labels2, soft_voting_predictions_70_30, \"Soft Voting 70-30\")"
      ],
      "metadata": {
        "id": "IOP4ibJDkJ-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# XLMRoberta+InceptionV3\n",
        "def soft_voting_predictions(text_probas, image_probas, weights):\n",
        "    combined_probas = (text_probas * weights[0] + image_probas * weights[1])\n",
        "    final_predictions = np.argmax(combined_probas, axis=1)\n",
        "    return final_predictions\n",
        "\n",
        "text_probas = np.array(XLMRoberta_probas)\n",
        "image_probas = InceptionV3_probabilities\n",
        "\n",
        "weights_50_50 = [0.5, 0.5]\n",
        "weights_60_40 = [0.6, 0.4]\n",
        "weights_70_30 = [0.7, 0.3]\n",
        "\n",
        "soft_voting_predictions_50_50 = soft_voting_predictions(text_probas, image_probas, weights_50_50)\n",
        "soft_voting_predictions_60_40 = soft_voting_predictions(text_probas, image_probas, weights_60_40)\n",
        "soft_voting_predictions_70_30 = soft_voting_predictions(text_probas, image_probas, weights_70_30)\n",
        "\n",
        "test_labels2 = np.argmax(test_labels, axis=1)\n",
        "def calculate_and_print_metrics(true_labels, predictions, model_name):\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    precision = precision_score(true_labels, predictions, average='macro')\n",
        "    recall = recall_score(true_labels, predictions, average='macro')\n",
        "    f1 = f1_score(true_labels, predictions, average='macro')\n",
        "    print(f\"{model_name} - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}\")\n",
        "\n",
        "print(\"XLMRoberta+InceptionV3\")\n",
        "calculate_and_print_metrics(test_labels2, soft_voting_predictions_50_50, \"Soft Voting 50-50\")\n",
        "calculate_and_print_metrics(test_labels2, soft_voting_predictions_60_40, \"Soft Voting 60-40\")\n",
        "calculate_and_print_metrics(test_labels2, soft_voting_predictions_70_30, \"Soft Voting 70-30\")"
      ],
      "metadata": {
        "id": "3ZomPZQAkf78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# XLMRoberta+EfficientNetV2S\n",
        "def soft_voting_predictions(text_probas, image_probas, weights):\n",
        "    combined_probas = (text_probas * weights[0] + image_probas * weights[1])\n",
        "    final_predictions = np.argmax(combined_probas, axis=1)\n",
        "    return final_predictions\n",
        "\n",
        "text_probas = np.array(XLMRoberta_probas)\n",
        "image_probas = EfficientNetV2S_probabilities\n",
        "\n",
        "weights_50_50 = [0.5, 0.5]\n",
        "weights_60_40 = [0.6, 0.4]\n",
        "weights_70_30 = [0.7, 0.3]\n",
        "\n",
        "soft_voting_predictions_50_50 = soft_voting_predictions(text_probas, image_probas, weights_50_50)\n",
        "soft_voting_predictions_60_40 = soft_voting_predictions(text_probas, image_probas, weights_60_40)\n",
        "soft_voting_predictions_70_30 = soft_voting_predictions(text_probas, image_probas, weights_70_30)\n",
        "\n",
        "test_labels2 = np.argmax(test_labels, axis=1)\n",
        "def calculate_and_print_metrics(true_labels, predictions, model_name):\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    precision = precision_score(true_labels, predictions, average='macro')\n",
        "    recall = recall_score(true_labels, predictions, average='macro')\n",
        "    f1 = f1_score(true_labels, predictions, average='macro')\n",
        "    print(f\"{model_name} - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}\")\n",
        "\n",
        "print(\"XLMRoberta+EfficientNetV2S\")\n",
        "calculate_and_print_metrics(test_labels2, soft_voting_predictions_50_50, \"Soft Voting 50-50\")\n",
        "calculate_and_print_metrics(test_labels2, soft_voting_predictions_60_40, \"Soft Voting 60-40\")\n",
        "calculate_and_print_metrics(test_labels2, soft_voting_predictions_70_30, \"Soft Voting 70-30\")"
      ],
      "metadata": {
        "id": "493OEQ1Sk8BR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# XLMRoberta+Xception\n",
        "def soft_voting_predictions(text_probas, image_probas, weights):\n",
        "    combined_probas = (text_probas * weights[0] + image_probas * weights[1])\n",
        "    final_predictions = np.argmax(combined_probas, axis=1)\n",
        "    return final_predictions\n",
        "\n",
        "text_probas = np.array(XLMRoberta_probas)\n",
        "image_probas = Xception_probabilities\n",
        "\n",
        "weights_50_50 = [0.5, 0.5]\n",
        "weights_60_40 = [0.6, 0.4]\n",
        "weights_70_30 = [0.7, 0.3]\n",
        "\n",
        "soft_voting_predictions_50_50 = soft_voting_predictions(text_probas, image_probas, weights_50_50)\n",
        "soft_voting_predictions_60_40 = soft_voting_predictions(text_probas, image_probas, weights_60_40)\n",
        "soft_voting_predictions_70_30 = soft_voting_predictions(text_probas, image_probas, weights_70_30)\n",
        "\n",
        "test_labels2 = np.argmax(test_labels, axis=1)\n",
        "def calculate_and_print_metrics(true_labels, predictions, model_name):\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    precision = precision_score(true_labels, predictions, average='macro')\n",
        "    recall = recall_score(true_labels, predictions, average='macro')\n",
        "    f1 = f1_score(true_labels, predictions, average='macro')\n",
        "    print(f\"{model_name} - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}\")\n",
        "\n",
        "print(\"XLMRoberta+Xception\")\n",
        "calculate_and_print_metrics(test_labels2, soft_voting_predictions_50_50, \"Soft Voting 50-50\")\n",
        "calculate_and_print_metrics(test_labels2, soft_voting_predictions_60_40, \"Soft Voting 60-40\")\n",
        "calculate_and_print_metrics(test_labels2, soft_voting_predictions_70_30, \"Soft Voting 70-30\")"
      ],
      "metadata": {
        "id": "inxUAXprlML2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 715500,
          "sourceId": 1246182,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 4034225,
          "sourceId": 7016524,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 4034344,
          "sourceId": 7016734,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30587,
      "isGpuEnabled": true,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}